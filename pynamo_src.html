<html>
  <head>
    <title>Pynamo: Exploring the Dynamo Paper in Python</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    <meta name="author" content="David Drysdale">
    <meta name="keywords" content="drysdale, Dynamo, Python">
    <link rel="stylesheet" href="pygments.css" />
    <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-23228806-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
    </script>
  </head>
  <body>
    <h1>Pynamo: Exploring the Dynamo Paper in Python</h1>
    <p>
      The
      <a href="http://s3.amazonaws.com/AllThingsDistributed/sosp/amazon-dynamo-sosp2007.pdf">original paper
      [PDF]</a> that describes Amazon's Dynamo system is extremely interesting, and clearly explained.  This
      page documents my attempt to create a bare-bones Python implementation of the key ideas from the Dynamo
      paper.
    </p>
    <p>
      My aim here is to use the process of writing code as a way of understanding the relevant ideas better
      &ndash; in other words the code here is
      <b>pedagogical</b> rather than practical.  For example,  I deliberately use inefficent
      implementations in places &ndash; I'd rather have a slow implementation that just uses the Python standard
      library, than a fast version with distractions (by either requiring additional <code>import</code>s, or
      by including implementations of data structure that aren't specific to Dynamo).
    </p>
    <p>
      I've also built the system on top of a simulated network, rather than on top of a real network or web
      server.  This means I can more easily test different scenarios and track what's going on (more on the
      framework <a href="#framework">at the end</a>).
    </p>
    <p>
      For my final proviso, I should point out that I'm not going to explain much about the original ideas in
      the Dynamo paper, which are generally explained perfectly clearly.  In other words, this page <b>assumes
      you're read the original paper</b> and that you have it close to hand when you're reading this.
    </p>
    <p>
      Structurally, we'll start by exploring some of the building blocks used in the design, then start
      building up the Dynamo software step by step.
    </p>

    <h3>Contents</h3>
    <ul>
      <li><a href="#consistent_hash">Building Block: Consistent Hashing (s4.2/s4.3)</a></li>
      <li><a href="#simple_put"><b>Put</b>ting It Together</a></li>
      <li><a href="#simple_get"><b>Get</b>ting it Back</a></li>
      <li><a href="#failure">Coping With Failure</a></li>
      <li><a href="#faildetect">Node Failure Detection (s4.8.3)</a></li>
      <li><a href="#failrecover">Node Recovery Detection</a></li>
      <li><a href="#hintedhandoff">Hinted Handoff</a></li>
      <li><a href="#merkle">Building Block: Merkle Trees (s4.7)</a></li>
      <li><a href="#catchup">Catchup</a></li>
      <li><a href="#vector_clock">Building Block: Vector Clocks (s4.4)</a></li>
      <li><a href="#divergence">Detecting Divergence</a></li>
      <li><a href="#divergence_repair">Repairing Divergence</a></li>

      <li><a href="#framework">Appendix: Node Simulation Framework</a></li>
    </ul>


    <a name="consistent_hash"><h2>Building Block: Consistent Hashing (s4.2/s4.3)</h2></a>
    <p>
      The first area to explore is the consistent hashing algorithm described in section 4.2 of the paper.
      The constructor for the <code>ConsistentHashTable</code> class builds up the list of nodes, and the
      <code>find_nodes</code> method returns the <i>preference list</i> of nodes for the key (where the first
      entry in the list is the <i>coordinator</i>).
    </p>
    <p>
      The first implementation just implements the straightforward one-hash-per-node approach:
    </p>
#include hash_simple.py
    <p>
      The paper indicates a couple of problems with this simplistic approach, which we can check by feeding
      in some random data:
    </p>
    <ul>
      <li><b>The distribution of hash values can be 'lumpy'</b>:
#result python hash_simple.py -s 1 HashSimpleTestCase.testDistribution
      </li>
      <li><b>All of the traffic of a failed node moves to a single other node</b>:
        Adding a node to the <code>avoid</code> parameter means that the (single) next node round the
        hash ring gets all of the keys that would have gone to the failed node.
      </li>
    </ul>
    <p>
      So we move to the second implementation, where each node gets multiple points in the hash ring, known
      as <i>virtual nodes</i>.  We implement this very simply, by adding a ":<i>&lt;count&gt;</i>" suffix to
      the string that we hash for the node position.
    </p>
#include hash_multiple.py
    <p>
      Let's see how much different this makes by feeding in some random data to a set of 50 nodes with 10
      copies of each node in the hash ring.
    </p>
    <ul>
      <li><b>Distribution of hash values</b>:
#result python hash_multiple.py -s 1 HashMultipleTestCase.testDistribution
      </li>
      <li><b>Distribution of traffic from a failed node</b>:
#result python hash_multiple.py -s 1 HashMultipleTestCase.testFailover
      </li>
   </ul>
    <p>
      For 50 nodes with <b>100</b> copies of each node in the hash ring.
    </p>
    <ul>
      <li><b>Distribution of hash values</b>:
#result python hash_multiple.py -s 1 -r 100 HashMultipleTestCase.testDistribution
      </li>
      <li><b>Distribution of traffic from a failed node</b>:
#result python hash_multiple.py -s 1 -r 100 HashMultipleTestCase.testFailover
      </li>
    </ul>
    <p>
      So the multiple node approach has lead to a much less 'lumpy' distribution of keys.
    </p>


    <a name="simple_put"><h2><b>Put</b>ting It Together</h2></a>
    <p>
      Now we're going to start to put everything together.  We're going to build on a simulated enviroment of
      nodes and messages between them, which allows testing and simulation.  The framework is described in
      more detail in the <a href="#framework">appendix</a>, but hopefully the framework's methods are clear
      enough not to need much explanation.
    </p>
    <p>
      First up, let's simulate a client that wants to use the Dynamo system.  Equivalently to section 4.1 of
      the paper, it has two methods: <code>put(key, context, value)</code> and <code>get(key)</code>.  The
      client is outside of the Dynamo system proper, so these methods are straightforward: build the
      appropriate message and send it to a random node in the Dynamo system (although this can be overridden
      with an explicit choice of node).
    </p>
#include dynamo1.py:clientnode
    <p>
      These messages are sent to a <code>DynamoNode</code>, whose definition includes some data structures.  The
      class includes some constants controlling the amount of replication, including the <b>N</b>, <b>W</b>
      and <b>R</b> parameters described in section 4.5 of the paper.  The class also keeps track of how many
      <code>DynamoNode</code> instances there are, and keeps a
      <a href="#consistent_hash">consistent hash table</a> that corresponds to those instances.
    </p>
#include dynamo1.py:dynamonode
    <p>
      Each instance of <code>DynamoNode</code> has some attributes of its own.  The <code>local_store</code>
      attribute is a Python dictionary that simulates the node's local data store; this is where the key/value
      pairs stored by the system end up.  There are also a <code>pending_put</code>
      and <code>pending_get</code> data structures that keep track of pending operations being coordinated by
      this node.
    </p>
    <p>
      Access to the local data store <code>local_store</code> is via the <code>store()</code>
      and <code>retrieve()</code> methods; this will allow us to substitute in more sophisticated
      functionality later.
    </p>
#include dynamo1.py:storage
    <p>
      So what happens when a client tries to <code>put()</code> a piece of data?  The initial request message
      will arrive at a random Dynamo node, and its first action is to figure out the <i>preference list</i>.
      <ul>
        <li>If the local node isn't in the preference list, it forwards the request on to a node that is in
          the list (specifically, the <i>coordinator</i> &ndash; the first node on the list).</li>
        <li>If the local node is in the preference list, then it sends a
          <b>Put</b> message to the N nodes (including itself) to get them to store the data.</li>
      </ul>
    </p>
#include dynamo1.py:rcv_clientput
    <p>
      Continuing to follow the progress of a <code>put()</code> operation, the behaviour of the nodes that
      receive the <b>Put</b> messages is straightforward: they store the value, and send a response to say
      they've done it.
    </p>
#include dynamo1.py:rcv_put
    <p>
      In turn, the original sending node ticks off these responses from its list of pending responses, and
      when it has had <b>W</b> total replies (including itself), the <code>put()</code> operation is done.
    </p>
#include dynamo1.py:rcv_putrsp
    <p>
      We can see an example of the whole process as a ladder diagram. Notice that the response goes back to
      the client before the final <b>Put</b> response arrives &ndash; the <b>N</b> parameter is 3, so
      the <b>Put</b> request is processed by 3 nodes, but the <b>W</b> parameter is only 2 and so the final
      response isn't needed for the process to complete.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_simple_put
    <p>
      Interspersing <b>Put</b> operations for two different keys, from two different clients makes the ladder
      diagram more complicated, but the underlying details are the same.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_double_put


    <a name="simple_get"><h2><b>Get</b>ting it Back</h2></a>
    <p>
      After seeing the <b>Put</b> infrastructure, the <b>Get</b> infrastructure is pretty similar.
    </p>
#include dynamo1.py:rcv_clientget
    <p>
      Each of those nodes simply retrieves the relevant information from its local data store and sends it
      back.
    </p>
#include dynamo1.py:rcv_get
    <p>
      Finally, when enough (i.e. <b>R</b>) responses arrive, the first Dynamo node sends a confirmation response
      back to the client.  This part has a slight complication &ndash; the results aren't necessarily all the same,
      so we return the whole set (folding duplicates).
    </p>
#include dynamo1.py:rcv_getrsp
    <p>
      The whole process looks like the following:
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_simple_get


    <a name="failure"><h2>Coping With Failure</h2></a>
    <p>
      Nothing we've written so far copes with failure.  If just one node were to fail, then because <b>N</b>
      is 3 and (say) <b>W</b> is 2, we might just cope:
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put1_fail_node2
    <p>
      But if two nodes fail, then we're doomed:
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put1_fail_nodes23
    <p>
      Almost everything we still need to add to the code is about engineering around the consequences of
      failures:
    </p>
    <ul>
      <li><b>Failure Detection</b>: To begin with, we need to be able to spot failed nodes.</li>
      <li><b>Expanded Preference Lists</b>: Once a failed node has been spotted, we need to extend the
        replication set to cope.</li>
      <li><b>Failure Recovery Detection</b>: A failed node can recover, and we need some way to detect
        this.</li>
      <li><b>Catchup</b>: When a node recovers after a failure, it is out-of-sync and needs to catch up with
        the rest of the network.</li>
      <li><b>Detecting Divergence</b>: If a failure (particularly a network partition) resulted in the values
        for a particular key having inconsistent values, we need to detect this,
        using <a href="#vector_clock">vector clocks</a>.</li>
      <li><b>Repairing Divergence</b>: Once the values for a key have diverged, the Dynamo network isn't able
        to figure out how to repair the divergence itself, and so it has to push the problem up to the client
        software.</li>
    </ul>

    <a name="faildetect"><h2>Node Failure Detection (Section 4.8.3)</h2></a>
    <p>
      The first step in coping with failure is detecting it.  The message-sending framework we're using can
      automatically start a timer for any request message that is sent, and after a suitable period without a
      response, it can notify the original sender of the failure.  To use this facility, the sender just needs
      to include a <code>rsp_timer_pop</code> method in its class.
    </p>
    <p>
      To illustrate, here's the changes to the client to get it to resend any request on failure.  If there
      are multiple pending requests for the same failed destination node, we assume that they will all also
      fail, and resend them too.
    </p>
#diff dynamo1.py:clientnode dynamo2.py:clientnode
    <p>
      To show this in action, we can see what happens if the first receiving node fails; the client eventually
      retries and picks a different node at random.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put2_fail_initial_node

    <p>
      Inside the Dynamo network, each node keeps track of the requests it has outstanding, and when a request
      times out the destination node is marked as failed and the request is sent on to additional (hopefully
      working) nodes.
    </p>
#diff dynamo1.py:dynamonode dynamo2.py:dynamonode
    <p>
      Failed nodes are avoided in the preference list, both for <b>Put</b> operations:
    </p>
#diff dynamo1.py:rcv_clientput dynamo2.py:rcv_clientput
    <p>and for <b>Get</b> operations:</p>
#diff dynamo1.py:rcv_clientget dynamo2.py:rcv_clientget
    <p>
      The code also needs to update the set of outstanding request when responses are received, both for <b>Put</b>:
    </p>
#diff dynamo1.py:rcv_putrsp dynamo2.py:rcv_putrsp
    <p>
      and <b>Get</b>:
    </p>
#diff dynamo1.py:rcv_getrsp dynamo2.py:rcv_getrsp
    <p>
      A node gets treated as failed when some request to it times out. To keep the replication factors up to
      scratch, the timeout code also expands the set of nodes that the original request was sent to.
    </p>
#include dynamo2.py:rsp_timer_pop
    <p>
      With these modifications, the failures shown earlier start to look more recoverable from:
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put2_fail_nodes23
    <p>
      A subsequent request for the same key will skip the failed nodes automatically:
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put2_fail_nodes23_2

    <a name="failrecover"><h2>Node Recovery Detection</h2></a>
    <p>
      Of course, nodes that have failed may recover.  To keep an eye out for this, we periodically check a
      node that has failed to see if it has recovered.
    </p>
#include dynamo.py:retry_failed_node
    <p>
      This <b>Ping</b> request will likely fail at first, which will keep the node on the failed list.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put2_fail_nodes23_3
    <p>
      But once a node recovers, some <b>Ping</b> will eventually succeed. However, a <b>Get</b> request after
      this recovery will not necessarily get the right answer: the different nodes that have been involved in
      storing values for the key along the way now have a different idea of what the most up-to-date value for
      that key is.  This means we'll need some way for the nodes to get back in sync.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put2_fail_nodes23_4a
    <p>However, at this point a subsequent <b>Put</b> request will return to using the original preference list.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put2_fail_nodes23_4b

    <a name="hintedhandoff"><h2>Hinted Handoff</h2></a>
    <p>
      The most straightforward approach to restore synchronization is a <i>hinted handoff</i>:
      the extra node that received the <b>Put</b> operation gets told about the failed node that should have
      received the <b>Put</b> in the first place.  The extra node can then monitor the original node for
      liveness, and when it recovers send it the data that it missed.
    </p>
    <p>
      The first change to do this is at the sending node, filling in a <code>handoff</code> parameter on
      the <b>Put</b> message that is sent to the extra nodes.
    </p>
#diff dynamo3.py:rcv_clientput dynamo4.py:rcv_clientput
    <p>
      The next change is at the receiving node, which explictly starts to monitor the failed nodes for
      recovery, and tracks which keys need to be propagated on recovery.
    </p>
#diff dynamo3.py:rcv_put dynamo4.py:rcv_put
    <p>
      This results in the generation of some additional <b>Ping</b> messages.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put2_fail_nodes23_5
    <p>
      Finally, when node recovery is detected, the appropriate <b>Put</b> messages to resynchronize the failed
      node's state are sent.
    </p>
#diff dynamo3.py:retry_failed_node dynamo4.py:retry_failed_node

    <p>
      This results in the recovered nodes discovering the state update that they'd missed out on.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_put2_fail_nodes23_6

    <a name="merkle"><h2>Building Block: Merkle Trees (s4.7)</h2></a>
    <p>
      A Merkle tree is a tree structure describing a data set where
    </p>
    <ul>
      <li>each leaf node of the tree has a value that is the hash of some subset of the data</li>
      <li>each branch node of the tree is a hash of its children.</li>
    </ul>
    <p>
      This allows differences between two versions of the same data set to be detected quickly:
      <ul>
        <li>If the roots of two trees have the same hash value, all of the nodes below them in the tree must
          be the same</li>
        <li>If the roots of two trees have different hash values, we can recursively examine their children to
          locate the differences</li>
        <li>If two leaf nodes have different values, their data subsets differ</li>
      </ul>
    <p>
      The core of this data structure, dealing with non-leaf nodes, is fairly generic:
    </p>
#include merkle.py:coretree
    <p>
      The overall tree and the leaf nodes are more tailored to our particular problem.  The data set we are
      trying to keep in sync is the key-value store at a Dynamo node &ndash; or to be more specific, a
      subrange of the keyspace for that store.  To allow the subranges to be manipulated, first we set up a
      utility function that maps the MD5 hash of the key onto 128-bit integers.
    </p>
#include merkle.py:keyhash
    <p>
      Our overall tree covers a particular subrange of keys, and if the tree's depth is <b>D</b> we will have
      2<sup>D</sup> leaf nodes, each of which covers an sub-subrange of the keyspace.
    </p>
#include merkle.py:leafnode
    <p>
      The overall <code>MerkleTree</code> class then builds the appropriate number of leaves, and their parents.
    </p>
#include merkle.py:tree
    <p>
      Given that we want to allow the data held in the tree to change, the ideal thing is for this data
      structure to appear like an ordinary Python <code>dict</code>.  Defining a useful <code>_findleaf</code>
      method allows us to set up all of the various container methods that
      <a href="http://docs.python.org/library/userdict.html#UserDict.DictMixin">allow this to happen</a>:
    </p>
#include merkle.py:container
    <p>
      With this, the Merkle tree looks like a normal <code>dict</code>.
    </p>
#python
from merkle import MerkleTree
mtree = MerkleTree()
mtree['a'] = 1
mtree['b'] = 2
print mtree['a']
print mtree.root.value.hexdigest()
del mtree['a']
print 'a' in mtree
print mtree.root.value.hexdigest()
#endpython


    <a name="catchup"><h2>Catchup</h2></a>
    ???

    <a name="vector_clock"><h2>Building Block: Vector Clocks (s4.4)</h2></a>
    <p>
      A vector clock is easy to implement; it's basically a dictionary whose keys are nodes and whose values
      are the last-seen sequence number for that node.
    </p>
#include vectorclock.py:coreclass
    <p>
      We can add entries to the vector clock, simulating different nodes and their counters, with one proviso: a
      node's counter isn't allowed to go backwards.
    </p>
#python
from vectorclock import VectorClock
v = VectorClock()
print v
v.update('A', 1)
print v
v.update('A', 3)
print v
v.update('B', 1001)
print v
v.update('B', 1002)
print v
v.update('B', 1)
#endpython
    <p>
      We can also define an ordering operation <b>&lt;</b> on vector clocks
      (together with all the other comparison operations) by adding the relevant
      <a href="http://www.python.org/dev/peps/pep-0207/">rich comparison</a> methods
      to the <code>VectorClock</code> class.
    </p>
#include vectorclock.py:comparisons
    <p>
      This is a <a href="http://en.wikipedia.org/wiki/Partially_ordered_set">partial order</a>:
    </p>
    <ul>
      <li>it's reflexive: <code>a &lt;= a</code> always holds (because <code>a==a</code>)</li>
      <li>it's antisymmetric: if <code>a &lt; b</code> and <code>b &lt; a</code>, then <code>a==b</code></li>
      <li>it's transitive: if <code>a &lt; b</code> and <code>b &lt; c</code>, then <code>a &lt;= c</code></li>
    </ul>
    <p>
      but it's not a <a href="http://en.wikipedia.org/wiki/Total_order">total order</a> &ndash; it's possible
      for neither <code>a &lt; b</code> nor <code>b &lt; a</code> to hold.
    </p>
#python
from vectorclock import VectorClock
v1 = VectorClock().update('A', 1).update('B', 2)
v2 = VectorClock().update('A', 2).update('B', 3)
print (v1 < v2)
v3 = VectorClock().update('X', 1).update('Y', 2)
print (v1 < v3)
print (v3 < v1)
#endpython
    <p>
      This partial order forms the basis for the more important thing we can do with vector clocks &ndash;
      combine them.
    </p>
#include vectorclock.py:coalesce
    <p>
      This operation folds together those vector clocks that are direct ancestors of each other, but
      keeping separate those that are not.
    </p>
#python
print v1, v2, v3
diverged_clocks = VectorClock.coalesce((v1, v2, v3))
for vc in diverged_clocks: print vc
print (diverged_clocks[0] < diverged_clocks[1])
print (diverged_clocks[1] < diverged_clocks[0])
#endpython
    <p>
      Finally, we need to be able to build a single vector clock that has an arbitrary set of direct
      ancestors; in other words, a way of reconverging a divergent set of vector clocks.
    </p>
#include vectorclock.py:converge
    <p>
      The Dynamo system doesn't (and can't) solve the general problem of resolving inconsistencies; that's
      pushed out to the code that uses the system, and which is aware of the meaning of the data that is
      opaque to Dynamo.  But when that application code has resolved an inconsistency,
      this <code>converge</code> method builds a vector clock that makes it clear that the inconsistency
      has been resolved.
    </p>
#python
converged_clock = VectorClock.converge(diverged_clocks)
print converged_clock
print (v1 < converged_clock)
print (v2 < converged_clock)
print (v3 < converged_clock)
#endpython
    <p>
      The Dynamo paper also discusses the problem that the size of vector clocks can become large as more and
      more nodes get involved in the history of changes to a particular key/value.  To get around this, they
      suggest keeping a timestamp along with the sequence number, and throwing away the oldest entry in a
      vector clock when it has more than 10 entries.  This is easily implemented as a subclass
      of <code>VectorClock</code>, but we won't bother using this variant from here on.
    </p>
#include vectorclockt.py
    <a name="divergence"><h2>Detecting Divergence</h2></a>
    <p>
      The vector clocks of the previous section allow Dynamo to detect when there have been distinct,
      inconsistent updates for a  particular key. The vector clock is held in the metadata associated with the
      key, and it is the responsibility of the Dynamo <b>client</b> to indicate the version of the data that
      it is updating.  In practical terms, this means that the client needs to preceded every <b>Put</b>
      operation with a <b>Get</b> operation to retrieve the appropriate metadata.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_get_put_get_put
    <p>
      For convenience, to cope with the situation where a client holds on to a key and makes multiple updates,
      we also return the new metadata on a <b>Put</b> response so it can be used for a subsequent <b>Put</b>:
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_get_put_put
    <p>
      The Dynamo node that acts as the coordinator for the key updates the vector clock with its own sequence
      numbers:
    </p>
    <p>
#diff dynamo4.py:rcv_clientput dynamo.py:rcv_clientput
    <p>
      For a straightforward node failure, the vector clock doesn't really add anything.  Continuing from the
      sequences above, a subsequent <b>Put</b> then a <b>Get</b> still return a single consistent value
      &ndash; albeit with a vector clock that indicates multiple Dynamo nodes have been involved in the
      history of this key.
    </p>
#bresult python test_dynamo.py --seed 10 SimpleTestCase.test_metadata_simple_fail
    <p>
      Of course, it's not just nodes that can fail &ndash; sometimes, the links between them fail too.  The
      worst scenario that this leads to is <i>network partition</i>: half of the network is on one side of the
      partition, the other half is on the other side, and never the twain shall meet.
      Network partitions are particularly bad because different clients can update keys on the either side of
      the partition, resulting in different, incompatible, values for the keys.
    </p>
#bresult python test_dynamo.py --seed 11 SimpleTestCase.test_partition
    <p>
      This is where the vector clock helps; after the network is repaired, the next <b>Get</b> operation
      returns a divergent set of metadata &ndash; <b><code>
#result python test_dynamo.py --seed 11 SimpleTestCase.test_partition_detect_metadata
      </code></b> indicating two possible values for the key, with two associated vector clocks.
      This indicates that there are inconsistent values for the key that
      need to be dealt with.
    </p>
#bresult python test_dynamo.py --seed 11 SimpleTestCase.test_partition_detect

    <a name="divergence_repair"><h2>Repairing Divergence</h2></a>
    <p>
      The data stored by Dynamo is opaque to the system, which means that Dynamo itself has no way of figuring
      out how to deal with the inconsistent data.  Therefore, the problem is pushed out to the client: the
      client has to figure it out, and the next <b>Put</b> message that includes a divergent set of vector
      clocks is assumed to subsume them all.
    </p>
#diff dynamo4.py:clientnode dynamo.py:clientnode
    <p>
      Following on from the sequences of the last section, this results in a <b>Put</b> operation with a
      single converged vector clock <b><code>
#result python test_dynamo.py --seed 11 SimpleTestCase.test_partition_restore_metadata
      </b></code>.
    </p>
#bresult python test_dynamo.py --seed 11 SimpleTestCase.test_partition_restore


    <a name="framework"><h2>Appendix: Node Simulation Framework</h2></a>
    <p>
      The code here is built on a framework that simulates nodes and the messages between them.  The modules
      for this are divided up as follows.
    </p>
    <ul>
      <li><code>framework.py</code>: Master code for scheduling and tracking nodes and messages, mostly in the
      <code>Framework</code> class.</li>
      <li><code>history.py</code>: Code to track a list of events (messages sent/received, timers start/stop
      etc) and generate sequence diagrams from them. Mostly encapsulated in the <code>History</code> class.</li>
      <li><code>timer.py</code>: Code to manage pending timers, via the <code>TimerManager</code> class.</li>
      <li><code>testutils.py</code>: Utilities for unit tests, mostly statistics (<code>Statistics</code> class).</li>
      <li><code>logconfig.py</code>: Initialization for logging.</li>
      <li><code>message.py</code>: Base classes for message structures.</li>
    </ul>
    <a name=""><h2>Epilog</h2></a>
    <p>
      A full copy of this project (text, source, scripts) can be downloaded
      at <a href="https://github.com/daviddrysdale/pynamo">GitHub</a> or downloads as a
      <a href="http://lurklurk.org/pynamo/pynamo.tgz">tarball here</a>.  The text is available under
      the <a href="http://www.gnu.org/licenses/fdl.html">GFDL 1.3</a>, the code is available under
      <a href="http://www.gnu.org/licenses/old-licenses/gpl-2.0.html">version 2 of the GPL</a>.
      <hr>
    <p>Copyright (c) 2010-2012, David Drysdale</p>

    <p>Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free
      Documentation License, Version 1.3 or any later version published by the Free Software Foundation; with
      no Invariant Sections, with no Front-Cover Texts, and with no Back-Cover Texts.  A copy of the license
      is available
      <a href="http://www.gnu.org/copyleft/fdl.html">here</a>.</p>
    <hr>
    <p><a href="http://www.lurklurk.org/">Back to Home Page</a></p>
    <hr>
    <p><a href="mailto:dmd at_sign_here lurklurk dot_here org">Contact me</a></p>
  </body>
</html>
