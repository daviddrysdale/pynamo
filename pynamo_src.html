<html>
<head>
<title>Exploring the Dynamo Paper in Python</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta name="author" content="David Drysdale">
<meta name="keywords" content="drysdale, Dynamo, Python">
<link rel="stylesheet" href="pygments.css" />
</head>
<body>
<h1>Exploring the Dynamo Paper in Python</h1>
<p>
Like many other people, I found the 
<a href="http://s3.amazonaws.com/AllThingsDistributed/sosp/amazon-dynamo-sosp2007.pdf">paper [PDF]</a> 
that describes Amazon's Dynamo system very interesting.
</p>
<p>
To fully understand something software-related, I always find it's best to actually write some code.
So I sat down to write an implementation of the key ideas from the Dynamo paper, in Python, and this
page holds the results.
</p>
<p>
In line with this original intent, I should point out that the code here is 
<b>pedagogical</b> rather than practical.  For example,  I deliberately use inefficent 
implementations in places &ndash; I'd rather have a slow implementation that just uses the Python standard
library, than a fast version with distractions (by either requiring additional <code>import</code>s, or
by including implementations of data structure that aren't specific to Dynamo).
</p>
<p>
I've also built the system on top of a simulated network, rather than on top of a real network or web server.
This means I can more easily test different scenarios and track what's going on (more on the framework
<a href="#framework">at the end</a>).
</p>
<p>
For my final proviso, I should point out that I'm not going to explain much of the original ideas in the
Dynamo paper.  It's a perfectly clear and readable paper, so there's no gain in re-explaining it, except for
the rare cases where some important details have been swept under the carpet.
</p>

<h3>Contents</h3>
<ul>
  <li><a href="#consistent_hash">Section 4.2/4.3: Partitioning Algorithm</a></li>
  <li><a href="#vector_clock">Section 4.4: Vector Clocks</a></li>
  <li><a href="#simple_put"><code>Put</code>ting It Together</a></li>
  <li><a href="#simple_get">Getting it Back</a></li>
  <li><a href="#failure">Coping With Failure</a></li>
  <li><a href="#framework">Appendix: Node Simulation Framework</a></li>
</ul>


<a name="consistent_hash"><h2>Section 4.2/4.3: Partitioning Algorithm</h2></a>
<p>
The first area to explore is the consistent hashing algorithm described in section 4.2 of the paper.
The constructor for the <code>ConsistentHashTable</code> builds up the list of nodes, and the 
<code>find_nodes</code> method returns the <i>preference list</i> of nodes for the key (where the first
entry in the list is the <i>coordinator</i>). 
</p>
<p>
The first implementation just implements the straightforward one-hash-per-node approach:
</p>
#include <hash_simple.py>
<p>
The paper indicates a couple of problems with this simplistic approach, which we can check by feeding in some
random data:
<ul>
  <li><b>The distribution of hash values can be 'lumpy'</b>: 
   #result <python hash_simple.py -s 1 HashSimpleTestCase.testDistribution>
  </li>
  <li><b>All of the traffic of a failed node moves to a single other node</b>:
    Adding a node to the <code>avoid</code> parameter means that the (single) next node round the 
    hash ring gets all of the keys that would have gone to the failed node.
  </li>
</ul>
</p>
<p>
So we move to the second implementation, where each node gets multiple points in the hash ring, known
as <i>virtual nodes</i>.  We implement this very simply, by adding a ":<i>&lt;count&gt;</i>" suffix to the
string that we hash for the node position.
</p>
#include <hash_multiple.py>
<p>
Let's see how much different this makes by feeding in some random data to a set of 50 nodes with 10 copies of
each node in the hash ring.
<ul>
  <li><b>Distribution of hash values</b>: 
   #result <python hash_multiple.py -s 1 HashMultipleTestCase.testDistribution>
  </li>
  <li><b>Distribution of traffic from a failed node</b>:
    #result <python hash_multiple.py -s 1 HashMultipleTestCase.testFailover>
  </li>
</ul>
For 50 nodes with <b>100</b> copies of each node in the hash ring.
<ul>
  <li><b>Distribution of hash values</b>: 
   #result <python hash_multiple.py -s 1 -r 100 HashMultipleTestCase.testDistribution>
  </li>
  <li><b>Distribution of traffic from a failed node</b>:
    #result <python hash_multiple.py -s 1 -r 100 HashMultipleTestCase.testFailover>
  </li>
</ul>


<a name="vector_clock"><h2>Section 4.4: Vector Clocks</h2></a>
<p>
A vector clock is easy to implement; it's basically a dictionary whose keys are nodes
and whose values are the last-seen sequence number for that node.
</p>
#include <vectorclock.py:coreclass>
<p>
We can add entries to the vector clock, simulating different nodes and their counters, with one proviso: a
node's counter isn't allowed to go backwards.
</p>
#python
from vectorclock import VectorClock
v = VectorClock()
print v
v.update('A', 1)
print v
v.update('A', 3)
print v
v.update('B', 1001)
print v
v.update('B', 1002)
print v
v.update('B', 1)
#endpython
<p>
We can also define an ordering operation <b>&lt;</b> on vector clocks
(together with all the other comparison operations) by adding the relevant 
<a href="http://www.python.org/dev/peps/pep-0207/">rich comparison</a> methods 
to the <code>VectorClock</code> class.
</p>
#include <vectorclock.py:comparisons>
<p>
This is a <a href="http://en.wikipedia.org/wiki/Partially_ordered_set">partial order</a>:
<ul>
  <li>it's reflexive: <code>a &lt;= a</code> always holds (because <code>a==a</code>)</li>
  <li>it's antisymmetric: if <code>a &lt; b</code> and <code>b &lt; a</code>, then <code>a==b</code></li>
  <li>it's transitive: if <code>a &lt; b</code> and <code>b &lt; c</code>, then <code>a &lt;= c</code></li>
</ul>
but it's not a <a href="http://en.wikipedia.org/wiki/Total_order">total order</a> &ndash; it's possible for neither <code>a &lt; b</code>
nor <code>b &lt; a</code> to hold.
</p>
#python
from vectorclock import VectorClock
v1 = VectorClock().update('A', 1).update('B', 2)
v2 = VectorClock().update('A', 2).update('B', 3)
print (v1 < v2)
v3 = VectorClock().update('X', 1).update('Y', 2)
print (v1 < v3)
print (v3 < v1)
#endpython
<p>
This partial order forms the basis for the more important thing we can do with vector clocks &ndash; combine
them.  
</p>
#include <vectorclock.py:coalesce>
<p>
This operation folds together those vector clocks that are direct ancestors of each other, but keeping separate those
that are not.
</p>
#python
diverged_clocks = VectorClock.coalesce((v1, v2, v3))
for vc in diverged_clocks: print vc
print (diverged_clocks[0] < diverged_clocks[1])
print (diverged_clocks[1] < diverged_clocks[0])
#endpython
<p>
Finally, we need to be able to build a single vector clock that has an arbitrary set of direct ancestors; in
other words, a way of reconverging a divergent set of vector clocks.
</p>
#include <vectorclock.py:converge>
<p>
Code that uses Dynamo needs to be able to resolve inconsistencies itself.  When it has done so, the new value
for the key needs a vector clock that makes it clear that the inconsistency has been resolved.
</p>
#python
converged_clock = VectorClock.converge(diverged_clocks)
print converged_clock
print (v1 < converged_clock)
print (v2 < converged_clock)
print (v3 < converged_clock)
#endpython
<p>
The Dynamo paper also discusses the problem that the size of vector clocks can become large as more and more
nodes get involved in the history of changes to a particular key/value.  To get around this, they suggest
keeping a timestamp along with the sequence number, and throwing away the oldest entry in a vector clock when
it has more than 10 entries.  This is easily implemented as a subclass of <code>VectorClock</code>, but we
won't bother using this variant from here on.
</p>
#include <vectorclockt.py>

<a name="simple_put"><h2><code>Put</code>ting It Together</h2></a>
<p>
Now we're going to start to put everything together.  We're going to build on a simulated enviroment of 
nodes and messages between them, which allows testing and simulation.  The framework is described in more detail in
the <a href="#framework">appendix</a>, but hopefully the framework's methods are clear enough not to
need much explanation.
</p>
<p>
First up, let's simulate a client that wants to use the Dynamo system.  Equivalently to section 4.1 of the
paper, it has two methods: <code>put(key, context, value)</code> and <code>get(key)</code>.
The client is outside of the Dynamo system proper, so these methods are straightforward: build the appropriate
message and send it to a random node in the Dynamo system (although this can be overridden with an explicit
choice of node).
</p>
#include <dynamo1.py:clientnode>
<p>
These messages are sent to a <code>DynamoNode</code>, whose definition includes some data structures.  The
class includes some constants controlling the amount of replication, including the <b>N</b>, <b>W</b>
and <b>R</b> parameters described in section 4.5 of the paper.  The class also keeps track of how many
<code>DynamoNode</code> instances there are, and keeps a 
<a href="#consistent_hash">consistent hash table</a> that corresponds to those instances.
</p>
#include <dynamo1.py:dynamonode>
<p>
Each instance of <code>DynamoNode</code> has some attributes of its own.  The <code>store</code> attribute is
a Python dictionary that simulates the node's local data store; this is where the key/value pairs stored by
the system end up.  There are also a <code>pending_put</code> and <code>pending_get</code> data structures
that keep track of pending operations being coordinated by this node.
</p>
<p>
So what happens when a client tries to <code>put()</code> a piece of data?  The initial request message will
arrive at a random Dynamo node, and its first action is to figure out the <i>preference list</i>.  
<ul>
  <li>If the local node isn't in the preference list, it forwards the request on to a node that is in the list (specifically, the 
    <i>coordinator</i> &ndash; the first node on the list).</li>
  <li>If the local node is in the preference list, then it stores the key/value/context locally, and sends a 
    <b>Put</b> message to N-1 other nodes to get them to store it too.</li>
</ul>
</p>
#include <dynamo1.py:rcv_clientput>
<p>
Continuing to follow the progress of a <code>put()</code> operation, the behaviour of the nodes that receive
the <b>Put</b> messages is straightforward: they store the value, and send a response to say they've done it.
</p>
#include <dynamo1.py:rcv_put>
<p>
In turn, the original sending node ticks off these responses from its list of pending responses, and when it
has had <b>W</b> total replies (including itself), the <code>put()</code> operation is done.
</p>
#include <dynamo1.py:rcv_putrsp>
<p>
We can see an example of the whole process as a ladder diagram. Notice that the response goes back to the
client before the final <b>Put</b> response arrives &ndash; the <b>N</b> parameter is 3, so the Put request is
processed by 3 nodes, but the <b>W</b> parameter is only 2 and so the final response isn't needed for the
process to complete.
</p>
<div class="highlight"><pre>
#result <python test_dynamo.py --seed 10 SimpleTestCase.test_simple_put></pre></div>
<p>
Interspersing Put operations for two different keys, from two different clients makes the ladder diagram more
complicated, but the underlying details are the same.
</p>
<div class="highlight"><pre>
#result <python test_dynamo.py --seed 10 SimpleTestCase.test_double_put></pre></div>


<a name="simple_get"><h2>Getting it Back</h2></a>
<p>
After seeing the <b>Put</b> infrastructure, the <b>Get</b> infrastructure is pretty straightforward.  The main
difference here is that whichever node first receives the client request coordinates the whole business,
whether or not it itself is on the preference list for the key.
</p>
<p>
So a client get request arrives at a node, which fans it out to <b>N</b> nodes on the preference list.
</p>
#include <dynamo1.py:rcv_clientget>
<p>
Each of those nodes simply retrieves the relevant information from its local data store and sends it back.
</p>
#include <dynamo1.py:rcv_get>
<p>
Finally, when enough (i.e. <b>R</b>) responses arrive, the first Dynamo node sends a confirmation response
back to the client.  This part has a slight complication &ndash; the results aren't necessarily all the same,
so we return the whole set (folding duplicates).
#include <dynamo1.py:rcv_getrsp>
<p>
The whole process looks like the following:
</p>
<div class="highlight"><pre>
#result <python test_dynamo.py --seed 10 SimpleTestCase.test_simple_get></pre></div>


<a name="failure"><h2>Coping With Failure</h2></a>
<p>
Nothing we've written so far copes with failure.  If just one node were to fail, then because <b>N</b> is 3
and (say) <b>W</b> is 2, we might just cope:
</p>
<div class="highlight"><pre>
#result <python test_dynamo.py --seed 10 SimpleTestCase.test_put1_fail_node2></pre></div>
<p>
But if two nodes fail, then we're doomed:
</p>
<div class="highlight"><pre>
#result <python test_dynamo.py --seed 10 SimpleTestCase.test_simple_put_fail4></pre></div>

<p>
The first step in coping with failure is detecting it.  The framework we're using can automatically start a timer
for any request message that is sent, and after a suitable period without a response, it can notify the
original sender of the failure.  To use this facility, the sender just needs to include a 
<code>rsp_timer_pop</code> method in its class.
</p>
<p>
To illustrate, here's the changes to the client to get it to resend any request on failure.
</p>
#diff <dynamo1.py:clientnode> <dynamo.py:clientnode>
<p>
To show this in action, we can see what happens if the first receiving node fails; the client eventually
retries and picks a different node at random.
</p>
<div class="highlight"><pre>
#result <python test_dynamo.py --seed 10 SimpleTestCase.test_put2_fail_initial_node></pre></div>
<p>
Inside the Dynamo network, each node keeps track of the nodes it believes to be failed.
</p>
#diff <dynamo1.py:dynamonode> <dynamo.py:dynamonode>
<p>
Failed nodes are avoided in the preference list, both for Put operations:
</p>
#diff <dynamo1.py:rcv_clientput> <dynamo.py:rcv_clientput>
<p>and for Get operations:</p>
#diff <dynamo1.py:rcv_clientget> <dynamo.py:rcv_clientget>
<p>A node gets treated as failed when some request to it times out. To keep the replication factors up to
  scratch, the timeout code also expands the set of nodes that the original request was sent to.</p>
#include <dynamo.py:rsp_timer_pop>
<p>
With these modifications, the failures shown earlier start to look more recoverable from:
</p>





??? node failure
 - failure of coordinator (1st node in preference list)

??? link failure

??? multiple node failure
??? node recovery


<a name="framework"><h2>Appendix: Node Simulation Framework</h2></a>

<ul>
  <li>framework.py: </li>
  <li>history.py: </li>
  <li>timer.py: </li>
  <li>testutils.py: </li>
  <li>logconfig.py: </li>
  <li>message.py: </li>
</ul>
<a name=""><h2>Epilog</h2></a>
<p>
A full copy of this project (text, source, scripts) can be 
<a href="pynamo.tgz">downloaded here</a>.  The text is available under
the <a href="http://www.gnu.org/licenses/fdl.html">GFDL 1.3</a>, the code is available under
<a href="http://www.gnu.org/licenses/old-licenses/gpl-2.0.html">version 2 of the GPL</a>.
<hr>
<p>Copyright (c) 2010-2011, David Drysdale</p>

<p>Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or any
later version published by the Free Software Foundation; with no Invariant
Sections, with no Front-Cover Texts, and with no Back-Cover Texts.  A copy
of the license is available 
<a href="http://www.gnu.org/copyleft/fdl.html">here</a>.</p>
<hr>
<p><a href="http://www.lurklurk.org/">Back to Home Page</a></p>
<hr>
<p><a href="mailto:dmd at_sign_here lurklurk dot_here org">Contact me</a></p>
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-707535-2";
urchinTracker();
</script>
</body>
</html>

